{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes/particlex/source/css/main.css","path":"css/main.css","modified":1,"renderable":1},{"_id":"themes/particlex/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"themes/particlex/source/images/avatar.jpg","path":"images/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/particlex/source/images/background.jpg","path":"images/background.jpg","modified":1,"renderable":1},{"_id":"themes/particlex/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/particlex/source/images/ff425b33-df90-4c42-9c6a-ea6fd43701b2.png","path":"images/ff425b33-df90-4c42-9c6a-ea6fd43701b2.png","modified":1,"renderable":1},{"_id":"themes/particlex/source/js/lib/crypto.js","path":"js/lib/crypto.js","modified":1,"renderable":1},{"_id":"themes/particlex/source/js/lib/highlight.js","path":"js/lib/highlight.js","modified":1,"renderable":1},{"_id":"themes/particlex/source/js/lib/home.js","path":"js/lib/home.js","modified":1,"renderable":1},{"_id":"themes/particlex/source/js/lib/math.js","path":"js/lib/math.js","modified":1,"renderable":1},{"_id":"themes/particlex/source/js/lib/preview.js","path":"js/lib/preview.js","modified":1,"renderable":1},{"_id":"themes/particlex/source/js/lib/search.js","path":"js/lib/search.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/_posts/BM_ques1.md","hash":"6275156f41949bf2b95295d29a95114bc94eb195","modified":1770456534439},{"_id":"source/codes/index.md","hash":"dcd0626022f3ac453d46113def9c60c57feea13f","modified":1770413941700},{"_id":"source/.DS_Store","hash":"262e4d12d5716e442dbc44ccc9802baa596cba52","modified":1770421467205},{"_id":"source/about/index.md","hash":"7f7273754f7ab9b264d0acb51dce4a9316b5c03d","modified":1770420237507},{"_id":"source/essays/index.md","hash":"03aed7797297ad4975eff0dc1bfb0476a454d3dd","modified":1770413939701},{"_id":"source/notes/index.md","hash":"e42fb0de92163f05c6e802c738aa2759f4dc6188","modified":1770413934395},{"_id":"source/projects/index.md","hash":"4f60aece6c2345d61526a872247411d28c0ad594","modified":1770413937200},{"_id":"themes/particlex/_config.yml","hash":"64fc32f11f467321f3af62e2417967cdd378fd82","modified":1770458425481},{"_id":"themes/particlex/package.json","hash":"0cc5bdbb51d74ba3e04a6adf4e34f233907c56af","modified":1770414885223},{"_id":"themes/particlex/.github/dependabot.yml","hash":"d2adc8df9ae7f28520d2fd5c0579e75dbe352bdc","modified":1770414885223},{"_id":"themes/particlex/layout/card.ejs","hash":"65e0c46a79c02212982c8a9aef1b16bfc3430429","modified":1770414885217},{"_id":"themes/particlex/layout/archives.ejs","hash":"c3878ffe435aa37ba46a0ee25c34257c2f2a1d87","modified":1770414885217},{"_id":"themes/particlex/README.md","hash":"19b124670bc5c9aa8cff0fc191be1e9a40c841a0","modified":1770414885223},{"_id":"themes/particlex/layout/categories.ejs","hash":"73ff20a582c72e7cf138c7b42006447e4a2e110a","modified":1770414885219},{"_id":"themes/particlex/layout/category.ejs","hash":"83e8f386d68a78afc9fe26c237734fef01f77d4d","modified":1770414910841},{"_id":"themes/particlex/layout/comment.ejs","hash":"267809e50962af7ab6bc5892855f765d754a62e4","modified":1770414885219},{"_id":"themes/particlex/layout/import.ejs","hash":"7800720150b46a9abaf513316602a05293b3991b","modified":1770414885218},{"_id":"themes/particlex/layout/footer.ejs","hash":"357b2db2eab031150e06dd47695dc9916d875fc4","modified":1770414885218},{"_id":"themes/particlex/layout/index.ejs","hash":"e7c208f1d9c0acb24f20180ffa45f48ee5ea5d71","modified":1770414885218},{"_id":"themes/particlex/layout/current.ejs","hash":"4e75c06c9d0b1336c69c210567581e7efded5621","modified":1770414885219},{"_id":"themes/particlex/layout/menu.ejs","hash":"1c78bd05cdfb3ae7a47ed572229a1970335fe8b8","modified":1770414885218},{"_id":"themes/particlex/layout/layout.ejs","hash":"ccea813da3c47c218e54a0b3c031dfd14b6db619","modified":1770463008439},{"_id":"themes/particlex/layout/post.ejs","hash":"a3e567ac00d1ccd8a84525d20665318248da75a7","modified":1770414885217},{"_id":"themes/particlex/layout/posts.ejs","hash":"18209210f37d7e8775ca54e4724ce66ad09d873f","modified":1770414885219},{"_id":"themes/particlex/LICENSE","hash":"22166681aaa54b0041e42147af8371053b8e7db8","modified":1770414885217},{"_id":"themes/particlex/layout/tags.ejs","hash":"f5b821d45f2f44443adade62eb032f92215e0d62","modified":1770414885219},{"_id":"themes/particlex/.DS_Store","hash":"18144d471a4fda25cd52e94d5e43722c7cb4e517","modified":1770458939555},{"_id":"themes/particlex/source/js/main.js","hash":"23a7d59d033da0e5ba3778ad020667c3395bb96f","modified":1770460124555},{"_id":"themes/particlex/source/css/main.css","hash":"ff49d90e3eb12de20b68a95c028a44d8d5f28a2e","modified":1770414885220},{"_id":"themes/particlex/source/images/loading.gif","hash":"9c840c5c3e7b97a184deb390df2f6926d6161708","modified":1770414885220},{"_id":"themes/particlex/source/js/lib/crypto.js","hash":"3db8692ac636d9f72dc94127216d21f9793e6602","modified":1770414885222},{"_id":"themes/particlex/source/js/lib/highlight.js","hash":"58fdb5f2d5e409bfc10aac6ccc464c87327806a5","modified":1770414885222},{"_id":"themes/particlex/source/js/lib/home.js","hash":"5ec113e1d72efaab5eb31addc05eb9dcf26ce1af","modified":1770414885222},{"_id":"themes/particlex/source/js/lib/math.js","hash":"f7716e83ef236818239fcae91defe730d5bfbc6d","modified":1770414885222},{"_id":"themes/particlex/source/.DS_Store","hash":"48dab3c082e9c29b45e388e099ce352e74b10027","modified":1770458939555},{"_id":"themes/particlex/source/images/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1770458949945},{"_id":"themes/particlex/source/js/lib/preview.js","hash":"d3050c6ed6d52e451cc1810843c5d595eadb8e5a","modified":1770414885222},{"_id":"themes/particlex/source/js/lib/search.js","hash":"d507facc680300c046f2b967279959541313e1f9","modified":1770414885222},{"_id":"themes/particlex/source/images/ff425b33-df90-4c42-9c6a-ea6fd43701b2.png","hash":"a8926457e8a98ebeaa6344f402da33ff6e920f75","modified":1770458909011},{"_id":"themes/particlex/source/images/avatar.jpg","hash":"d3d635a855685aac36d450a553fb7b88aeeafcf6","modified":1770418894494},{"_id":"themes/particlex/source/images/background.jpg","hash":"3b78412db423a1fdbfc2aeadc6f653d95a707dab","modified":1770418949042},{"_id":"public/about/index.html","hash":"6ef818d01b1b16548bdbe505fc6139df10240014","modified":1770463054903},{"_id":"public/codes/index.html","hash":"e30e35def083db01712801e1b05a85acbd2f3596","modified":1770463054903},{"_id":"public/essays/index.html","hash":"51c226392d70619dc46d7a5b5bc6d45a974a5ef4","modified":1770463054903},{"_id":"public/projects/index.html","hash":"386c5ace0026a1dd3b5c584dd4f32a630ba14c74","modified":1770463054903},{"_id":"public/notes/index.html","hash":"25130f8a95ebee672080b1818bda3ad740c65bb4","modified":1770463054903},{"_id":"public/notes/BM_ques1/index.html","hash":"0f9d119717b7a845a82997d3f4a40087062e4001","modified":1770463054903},{"_id":"public/archives/index.html","hash":"c724d61f7ec964f86b13ad2f87b5f8d24fe760cf","modified":1770463054903},{"_id":"public/archives/2026/index.html","hash":"f3a9b07c13fa866adcefbdd25188db5cfa6be99b","modified":1770463054903},{"_id":"public/archives/2026/02/index.html","hash":"02dfe25965fe3d16fafcc8068cf9bf1a92a04ec1","modified":1770463054903},{"_id":"public/categories/notes/index.html","hash":"fce10b43d2cfc019f2cb07d4c7e5cfe4f7ec3869","modified":1770463054903},{"_id":"public/index.html","hash":"1fa99b8d3c4d84eb2e0be16e5af648b19889013a","modified":1770463054903},{"_id":"public/tags/Probability/index.html","hash":"5cc9f57ce5747ca394f388910fe6acf045e51f91","modified":1770463054903},{"_id":"public/tags/Stochastic-Processes/index.html","hash":"eb6e5ab52a688cbe64e6e8700f6b77a3ae04ca49","modified":1770463054903},{"_id":"public/tags/Brownian-Motion/index.html","hash":"54c6dc5a0c4e1422a5a825b70e1bf032fa09aa12","modified":1770463054903},{"_id":"public/images/loading.gif","hash":"9c840c5c3e7b97a184deb390df2f6926d6161708","modified":1770463054903},{"_id":"public/css/main.css","hash":"444ff8856c320913dde037c43f24bba18ef4d741","modified":1770463054903},{"_id":"public/js/main.js","hash":"96c8e405c308bdfe7d6da5d12b8204be420f681e","modified":1770463054903},{"_id":"public/js/lib/crypto.js","hash":"bc4a0c41cf5b61faa204a2a820fc042b563142cf","modified":1770463054903},{"_id":"public/js/lib/home.js","hash":"c2bf22772fd052cff88a9b5f547a30a6eb97e545","modified":1770463054903},{"_id":"public/js/lib/highlight.js","hash":"a9ee0fd40904e2e50ab5ecab4c718a49c095836f","modified":1770463054903},{"_id":"public/js/lib/math.js","hash":"24c182cd3f5dd1c0f0192ca4cc143de71e076d2a","modified":1770463054903},{"_id":"public/js/lib/search.js","hash":"b631b87fa126a9a4a81b60b1a0516f765879963e","modified":1770463054903},{"_id":"public/js/lib/preview.js","hash":"595cfc3aff107b8dd0fdda214995c6f1bb5be39a","modified":1770463054903},{"_id":"public/images/ff425b33-df90-4c42-9c6a-ea6fd43701b2.png","hash":"a8926457e8a98ebeaa6344f402da33ff6e920f75","modified":1770463054903},{"_id":"public/images/avatar.jpg","hash":"d3d635a855685aac36d450a553fb7b88aeeafcf6","modified":1770463054903},{"_id":"public/images/background.jpg","hash":"3b78412db423a1fdbfc2aeadc6f653d95a707dab","modified":1770463054903}],"Category":[{"name":"notes","_id":"cmlc7zo8f00068q0m23sh30ta"}],"Data":[],"Page":[{"title":"About","date":"2026-02-05T23:00:00.000Z","layout":"page","_content":"\n# About Me\n\n## Introduction\n\nI am currently pursuing an **MSc in Mathematics** at the **University of Copenhagen**. My research interests span across multiple domains, with a particular focus on the intersection of mathematics, statistics, and computational methods.\n\n### Research Interests\n\nMy work centers on understanding causal relationships and developing robust statistical methods:\n\n- **Causal Inference**: Developing and applying methods to understand cause-and-effect relationships in complex systems\n- **Causal Discovery**: Algorithmic approaches to learning causal structures from observational data\n- **High-Dimensional Statistics**: Statistical methods for analyzing data with many variables\n- **Narrative Systems**: Exploring the mathematical structure of narratives and storytelling\n- **Digital Humanities**: Applying computational and statistical methods to humanities research\n\n## Contact\n\n- **Email**: `<YOUR_EMAIL>`\n- **GitHub**: [<YOUR_GITHUB>](https://github.com/<YOUR_GITHUB>)\n- **Google Scholar**: [Your Profile](https://scholar.google.com/) *(optional)*\n- **ORCID**: [Your ORCID](https://orcid.org/) *(optional)*\n\n---\n\n## Curriculum Vitae\n\n### Education\n\n**MSc Mathematics** | University of Copenhagen | *2024 - Present*\n- Focus: Causal Inference, High-Dimensional Statistics\n- Relevant Coursework: [List key courses]\n\n**[Your Bachelor's Degree]** | [Southwest Jiaotong University] | *[2021 - 2025]*\n- [Brief description or honors]\n\n### Research Experience\n\n**[Project/Position Title]** | [Institution] | *[Dates]*\n- [Brief description of research work]\n- [Key achievements or publications]\n\n### Projects\n\nSee my [Projects](/projects/) page for detailed information about ongoing and completed research projects.\n\n### Publications\n\n*(Optional - Add your publications here)*\n\n1. **[Paper Title]**. *Authors*. Journal/Conference, Year. [Link]\n\n### Teaching & Mentoring\n\n*(Optional)*\n\n- **[Course Name]** | [Role] | [Institution] | *[Term/Year]*\n\n### Skills\n\n**Programming & Tools**\n- Python, R\n- Statistical Computing: NumPy, Pandas, scikit-learn\n- Causal Inference: DoWhy, CausalML, bnlearn\n- Version Control: Git, GitHub\n\n**Statistical Methods**\n- Causal inference and discovery\n- Bayesian statistics\n- High-dimensional data analysis\n- Experimental design\n\n**Languages**\n- English (Fluent)\n- [Your native language]\n- [Other languages]\n\n---\n\n### Download CV\n\nIf you prefer a PDF version of my CV, you can download it here:\n\n[Download CV (PDF)](/files/CV.pdf) *(Optional - only if you have a CV.pdf file)*\n\n---\n\n*Last updated: February 2026*\n","source":"about/index.md","raw":"---\ntitle: About\ndate: 2026-02-06\nlayout: page\n---\n\n# About Me\n\n## Introduction\n\nI am currently pursuing an **MSc in Mathematics** at the **University of Copenhagen**. My research interests span across multiple domains, with a particular focus on the intersection of mathematics, statistics, and computational methods.\n\n### Research Interests\n\nMy work centers on understanding causal relationships and developing robust statistical methods:\n\n- **Causal Inference**: Developing and applying methods to understand cause-and-effect relationships in complex systems\n- **Causal Discovery**: Algorithmic approaches to learning causal structures from observational data\n- **High-Dimensional Statistics**: Statistical methods for analyzing data with many variables\n- **Narrative Systems**: Exploring the mathematical structure of narratives and storytelling\n- **Digital Humanities**: Applying computational and statistical methods to humanities research\n\n## Contact\n\n- **Email**: `<YOUR_EMAIL>`\n- **GitHub**: [<YOUR_GITHUB>](https://github.com/<YOUR_GITHUB>)\n- **Google Scholar**: [Your Profile](https://scholar.google.com/) *(optional)*\n- **ORCID**: [Your ORCID](https://orcid.org/) *(optional)*\n\n---\n\n## Curriculum Vitae\n\n### Education\n\n**MSc Mathematics** | University of Copenhagen | *2024 - Present*\n- Focus: Causal Inference, High-Dimensional Statistics\n- Relevant Coursework: [List key courses]\n\n**[Your Bachelor's Degree]** | [Southwest Jiaotong University] | *[2021 - 2025]*\n- [Brief description or honors]\n\n### Research Experience\n\n**[Project/Position Title]** | [Institution] | *[Dates]*\n- [Brief description of research work]\n- [Key achievements or publications]\n\n### Projects\n\nSee my [Projects](/projects/) page for detailed information about ongoing and completed research projects.\n\n### Publications\n\n*(Optional - Add your publications here)*\n\n1. **[Paper Title]**. *Authors*. Journal/Conference, Year. [Link]\n\n### Teaching & Mentoring\n\n*(Optional)*\n\n- **[Course Name]** | [Role] | [Institution] | *[Term/Year]*\n\n### Skills\n\n**Programming & Tools**\n- Python, R\n- Statistical Computing: NumPy, Pandas, scikit-learn\n- Causal Inference: DoWhy, CausalML, bnlearn\n- Version Control: Git, GitHub\n\n**Statistical Methods**\n- Causal inference and discovery\n- Bayesian statistics\n- High-dimensional data analysis\n- Experimental design\n\n**Languages**\n- English (Fluent)\n- [Your native language]\n- [Other languages]\n\n---\n\n### Download CV\n\nIf you prefer a PDF version of my CV, you can download it here:\n\n[Download CV (PDF)](/files/CV.pdf) *(Optional - only if you have a CV.pdf file)*\n\n---\n\n*Last updated: February 2026*\n","updated":"2026-02-06T23:23:57.507Z","path":"about/index.html","comments":1,"_id":"cmlc7zo8c00008q0md6pnepmv","content":"<h1 id=\"about-me\">About Me</h1>\n<h2 id=\"introduction\">Introduction</h2>\n<p>I am currently pursuing an <strong>MSc in Mathematics</strong> at the <strong>University of Copenhagen</strong>. My research interests span across multiple domains, with a particular focus on the intersection of mathematics, statistics, and computational methods.</p>\n<h3 id=\"research-interests\">Research Interests</h3>\n<p>My work centers on understanding causal relationships and developing robust statistical methods:</p>\n<ul>\n<li><strong>Causal Inference</strong>: Developing and applying methods to understand cause-and-effect relationships in complex systems</li>\n<li><strong>Causal Discovery</strong>: Algorithmic approaches to learning causal structures from observational data</li>\n<li><strong>High-Dimensional Statistics</strong>: Statistical methods for analyzing data with many variables</li>\n<li><strong>Narrative Systems</strong>: Exploring the mathematical structure of narratives and storytelling</li>\n<li><strong>Digital Humanities</strong>: Applying computational and statistical methods to humanities research</li>\n</ul>\n<h2 id=\"contact\">Contact</h2>\n<ul>\n<li><strong>Email</strong>: <code>&lt;YOUR_EMAIL&gt;</code></li>\n<li><strong>GitHub</strong>: <a href=\"https://github.com/%3CYOUR_GITHUB%3E\"><YOUR_GITHUB></a></li>\n<li><strong>Google Scholar</strong>: <a href=\"https://scholar.google.com/\">Your Profile</a> <em>(optional)</em></li>\n<li><strong>ORCID</strong>: <a href=\"https://orcid.org/\">Your ORCID</a> <em>(optional)</em></li>\n</ul>\n<hr />\n<h2 id=\"curriculum-vitae\">Curriculum Vitae</h2>\n<h3 id=\"education\">Education</h3>\n<p><strong>MSc Mathematics</strong> | University of Copenhagen | <em>2024 - Present</em> - Focus: Causal Inference, High-Dimensional Statistics - Relevant Coursework: [List key courses]</p>\n<p><strong>[Your Bachelor‚Äôs Degree]</strong> | [Southwest Jiaotong University] | <em>[2021 - 2025]</em> - [Brief description or honors]</p>\n<h3 id=\"research-experience\">Research Experience</h3>\n<p><strong>[Project/Position Title]</strong> | [Institution] | <em>[Dates]</em> - [Brief description of research work] - [Key achievements or publications]</p>\n<h3 id=\"projects\">Projects</h3>\n<p>See my <a href=\"/projects/\">Projects</a> page for detailed information about ongoing and completed research projects.</p>\n<h3 id=\"publications\">Publications</h3>\n<p><em>(Optional - Add your publications here)</em></p>\n<ol type=\"1\">\n<li><strong>[Paper Title]</strong>. <em>Authors</em>. Journal/Conference, Year. [Link]</li>\n</ol>\n<h3 id=\"teaching-mentoring\">Teaching &amp; Mentoring</h3>\n<p><em>(Optional)</em></p>\n<ul>\n<li><strong>[Course Name]</strong> | [Role] | [Institution] | <em>[Term/Year]</em></li>\n</ul>\n<h3 id=\"skills\">Skills</h3>\n<p><strong>Programming &amp; Tools</strong> - Python, R - Statistical Computing: NumPy, Pandas, scikit-learn - Causal Inference: DoWhy, CausalML, bnlearn - Version Control: Git, GitHub</p>\n<p><strong>Statistical Methods</strong> - Causal inference and discovery - Bayesian statistics - High-dimensional data analysis - Experimental design</p>\n<p><strong>Languages</strong> - English (Fluent) - [Your native language] - [Other languages]</p>\n<hr />\n<h3 id=\"download-cv\">Download CV</h3>\n<p>If you prefer a PDF version of my CV, you can download it here:</p>\n<p><a href=\"/files/CV.pdf\">Download CV (PDF)</a> <em>(Optional - only if you have a CV.pdf file)</em></p>\n<hr />\n<p><em>Last updated: February 2026</em></p>\n","excerpt":"","more":"<h1 id=\"about-me\">About Me</h1>\n<h2 id=\"introduction\">Introduction</h2>\n<p>I am currently pursuing an <strong>MSc in Mathematics</strong> at the <strong>University of Copenhagen</strong>. My research interests span across multiple domains, with a particular focus on the intersection of mathematics, statistics, and computational methods.</p>\n<h3 id=\"research-interests\">Research Interests</h3>\n<p>My work centers on understanding causal relationships and developing robust statistical methods:</p>\n<ul>\n<li><strong>Causal Inference</strong>: Developing and applying methods to understand cause-and-effect relationships in complex systems</li>\n<li><strong>Causal Discovery</strong>: Algorithmic approaches to learning causal structures from observational data</li>\n<li><strong>High-Dimensional Statistics</strong>: Statistical methods for analyzing data with many variables</li>\n<li><strong>Narrative Systems</strong>: Exploring the mathematical structure of narratives and storytelling</li>\n<li><strong>Digital Humanities</strong>: Applying computational and statistical methods to humanities research</li>\n</ul>\n<h2 id=\"contact\">Contact</h2>\n<ul>\n<li><strong>Email</strong>: <code>&lt;YOUR_EMAIL&gt;</code></li>\n<li><strong>GitHub</strong>: <a href=\"https://github.com/%3CYOUR_GITHUB%3E\"><YOUR_GITHUB></a></li>\n<li><strong>Google Scholar</strong>: <a href=\"https://scholar.google.com/\">Your Profile</a> <em>(optional)</em></li>\n<li><strong>ORCID</strong>: <a href=\"https://orcid.org/\">Your ORCID</a> <em>(optional)</em></li>\n</ul>\n<hr />\n<h2 id=\"curriculum-vitae\">Curriculum Vitae</h2>\n<h3 id=\"education\">Education</h3>\n<p><strong>MSc Mathematics</strong> | University of Copenhagen | <em>2024 - Present</em> - Focus: Causal Inference, High-Dimensional Statistics - Relevant Coursework: [List key courses]</p>\n<p><strong>[Your Bachelor‚Äôs Degree]</strong> | [Southwest Jiaotong University] | <em>[2021 - 2025]</em> - [Brief description or honors]</p>\n<h3 id=\"research-experience\">Research Experience</h3>\n<p><strong>[Project/Position Title]</strong> | [Institution] | <em>[Dates]</em> - [Brief description of research work] - [Key achievements or publications]</p>\n<h3 id=\"projects\">Projects</h3>\n<p>See my <a href=\"/projects/\">Projects</a> page for detailed information about ongoing and completed research projects.</p>\n<h3 id=\"publications\">Publications</h3>\n<p><em>(Optional - Add your publications here)</em></p>\n<ol type=\"1\">\n<li><strong>[Paper Title]</strong>. <em>Authors</em>. Journal/Conference, Year. [Link]</li>\n</ol>\n<h3 id=\"teaching-mentoring\">Teaching &amp; Mentoring</h3>\n<p><em>(Optional)</em></p>\n<ul>\n<li><strong>[Course Name]</strong> | [Role] | [Institution] | <em>[Term/Year]</em></li>\n</ul>\n<h3 id=\"skills\">Skills</h3>\n<p><strong>Programming &amp; Tools</strong> - Python, R - Statistical Computing: NumPy, Pandas, scikit-learn - Causal Inference: DoWhy, CausalML, bnlearn - Version Control: Git, GitHub</p>\n<p><strong>Statistical Methods</strong> - Causal inference and discovery - Bayesian statistics - High-dimensional data analysis - Experimental design</p>\n<p><strong>Languages</strong> - English (Fluent) - [Your native language] - [Other languages]</p>\n<hr />\n<h3 id=\"download-cv\">Download CV</h3>\n<p>If you prefer a PDF version of my CV, you can download it here:</p>\n<p><a href=\"/files/CV.pdf\">Download CV (PDF)</a> <em>(Optional - only if you have a CV.pdf file)</em></p>\n<hr />\n<p><em>Last updated: February 2026</em></p>\n"},{"title":"Code Repositories","date":"2026-02-05T23:00:00.000Z","type":"codes","layout":"category","_content":"\n# Code Repositories\n\nThis page indexes my code repositories, reproducible scripts, and software tools. Each entry includes usage instructions, dependencies, and example outputs.\n\n---\n\nBrowse all code repositories below:\n","source":"codes/index.md","raw":"---\ntitle: Code Repositories\ndate: 2026-02-06\ntype: \"codes\"\nlayout: \"category\"\n---\n\n# Code Repositories\n\nThis page indexes my code repositories, reproducible scripts, and software tools. Each entry includes usage instructions, dependencies, and example outputs.\n\n---\n\nBrowse all code repositories below:\n","updated":"2026-02-06T21:39:01.700Z","path":"codes/index.html","comments":1,"_id":"cmlc7zo8d00018q0mdkts5mg2","content":"<h1 id=\"code-repositories\">Code Repositories</h1>\n<p>This page indexes my code repositories, reproducible scripts, and software tools. Each entry includes usage instructions, dependencies, and example outputs.</p>\n<hr />\n<p>Browse all code repositories below:</p>\n","excerpt":"","more":"<h1 id=\"code-repositories\">Code Repositories</h1>\n<p>This page indexes my code repositories, reproducible scripts, and software tools. Each entry includes usage instructions, dependencies, and example outputs.</p>\n<hr />\n<p>Browse all code repositories below:</p>\n"},{"title":"Essays","date":"2026-02-05T23:00:00.000Z","type":"essays","layout":"category","_content":"\n# Essays\n\nThis section contains longer-form writing on methodology, interdisciplinary connections, and reflections on research and learning. Essays explore broader themes and synthesize ideas across different domains.\n\n---\n\nBrowse all essays below:\n","source":"essays/index.md","raw":"---\ntitle: Essays\ndate: 2026-02-06\ntype: \"essays\"\nlayout: \"category\"\n---\n\n# Essays\n\nThis section contains longer-form writing on methodology, interdisciplinary connections, and reflections on research and learning. Essays explore broader themes and synthesize ideas across different domains.\n\n---\n\nBrowse all essays below:\n","updated":"2026-02-06T21:38:59.701Z","path":"essays/index.html","comments":1,"_id":"cmlc7zo8d00028q0mf16be7x0","content":"<h1 id=\"essays\">Essays</h1>\n<p>This section contains longer-form writing on methodology, interdisciplinary connections, and reflections on research and learning. Essays explore broader themes and synthesize ideas across different domains.</p>\n<hr />\n<p>Browse all essays below:</p>\n","excerpt":"","more":"<h1 id=\"essays\">Essays</h1>\n<p>This section contains longer-form writing on methodology, interdisciplinary connections, and reflections on research and learning. Essays explore broader themes and synthesize ideas across different domains.</p>\n<hr />\n<p>Browse all essays below:</p>\n"},{"title":"Projects","date":"2026-02-05T23:00:00.000Z","type":"projects","layout":"category","_content":"\n# Research Projects\n\nThis page showcases my ongoing and completed research projects. Each project includes its current status, objectives, and links to repositories where applicable.\n\n## Project Status Legend\n\n- üü¢ **Ongoing**: Currently active\n- ‚úÖ **Done**: Completed\n- ‚è∏Ô∏è **Paused**: Temporarily on hold\n\n---\n\nBrowse all projects below:\n","source":"projects/index.md","raw":"---\ntitle: Projects\ndate: 2026-02-06\ntype: \"projects\"\nlayout: \"category\"\n---\n\n# Research Projects\n\nThis page showcases my ongoing and completed research projects. Each project includes its current status, objectives, and links to repositories where applicable.\n\n## Project Status Legend\n\n- üü¢ **Ongoing**: Currently active\n- ‚úÖ **Done**: Completed\n- ‚è∏Ô∏è **Paused**: Temporarily on hold\n\n---\n\nBrowse all projects below:\n","updated":"2026-02-06T21:38:57.200Z","path":"projects/index.html","comments":1,"_id":"cmlc7zo8d00038q0mbjsn2gt3","content":"<h1 id=\"research-projects\">Research Projects</h1>\n<p>This page showcases my ongoing and completed research projects. Each project includes its current status, objectives, and links to repositories where applicable.</p>\n<h2 id=\"project-status-legend\">Project Status Legend</h2>\n<ul>\n<li>üü¢ <strong>Ongoing</strong>: Currently active</li>\n<li>‚úÖ <strong>Done</strong>: Completed</li>\n<li>‚è∏Ô∏è <strong>Paused</strong>: Temporarily on hold</li>\n</ul>\n<hr />\n<p>Browse all projects below:</p>\n","excerpt":"","more":"<h1 id=\"research-projects\">Research Projects</h1>\n<p>This page showcases my ongoing and completed research projects. Each project includes its current status, objectives, and links to repositories where applicable.</p>\n<h2 id=\"project-status-legend\">Project Status Legend</h2>\n<ul>\n<li>üü¢ <strong>Ongoing</strong>: Currently active</li>\n<li>‚úÖ <strong>Done</strong>: Completed</li>\n<li>‚è∏Ô∏è <strong>Paused</strong>: Temporarily on hold</li>\n</ul>\n<hr />\n<p>Browse all projects below:</p>\n"},{"title":"Notes","date":"2026-02-05T23:00:00.000Z","type":"notes","layout":"category","_content":"\n# Notes\n\nThis page contains my learning notes, concept explorations, derivations, and short research memos. Notes are organized by topic rather than by course, focusing on understanding fundamental concepts and building intuition.\n\n## Topics\n\n- **Causality**: Causal inference methods, identification, estimation\n- **Probability**: Probability theory, stochastic processes\n- **Statistics**: Statistical inference, hypothesis testing, estimation\n- **Meta**: Reflections on learning, research methodology\n\n---\n\nBrowse all notes below:\n","source":"notes/index.md","raw":"---\ntitle: Notes\ndate: 2026-02-06\ntype: \"notes\"\nlayout: \"category\"\n---\n\n# Notes\n\nThis page contains my learning notes, concept explorations, derivations, and short research memos. Notes are organized by topic rather than by course, focusing on understanding fundamental concepts and building intuition.\n\n## Topics\n\n- **Causality**: Causal inference methods, identification, estimation\n- **Probability**: Probability theory, stochastic processes\n- **Statistics**: Statistical inference, hypothesis testing, estimation\n- **Meta**: Reflections on learning, research methodology\n\n---\n\nBrowse all notes below:\n","updated":"2026-02-06T21:38:54.395Z","path":"notes/index.html","comments":1,"_id":"cmlc7zo8d00048q0mcyp092f6","content":"<h1 id=\"notes\">Notes</h1>\n<p>This page contains my learning notes, concept explorations, derivations, and short research memos. Notes are organized by topic rather than by course, focusing on understanding fundamental concepts and building intuition.</p>\n<h2 id=\"topics\">Topics</h2>\n<ul>\n<li><strong>Causality</strong>: Causal inference methods, identification, estimation</li>\n<li><strong>Probability</strong>: Probability theory, stochastic processes</li>\n<li><strong>Statistics</strong>: Statistical inference, hypothesis testing, estimation</li>\n<li><strong>Meta</strong>: Reflections on learning, research methodology</li>\n</ul>\n<hr />\n<p>Browse all notes below:</p>\n","excerpt":"","more":"<h1 id=\"notes\">Notes</h1>\n<p>This page contains my learning notes, concept explorations, derivations, and short research memos. Notes are organized by topic rather than by course, focusing on understanding fundamental concepts and building intuition.</p>\n<h2 id=\"topics\">Topics</h2>\n<ul>\n<li><strong>Causality</strong>: Causal inference methods, identification, estimation</li>\n<li><strong>Probability</strong>: Probability theory, stochastic processes</li>\n<li><strong>Statistics</strong>: Statistical inference, hypothesis testing, estimation</li>\n<li><strong>Meta</strong>: Reflections on learning, research methodology</li>\n</ul>\n<hr />\n<p>Browse all notes below:</p>\n"}],"Post":[{"title":"Brownian Motion - Existence Proof","date":"2026-02-05T23:00:00.000Z","course":null,"_content":"\n# Brownian Motion\n\nExistence of a probabilistic Brownian motion\n\n<!-- more -->\n\n## Statement: Ê†∏ÂøÉÂÆö‰πâ (Statements)\n\nWe know the definition of Brownian motion as below:\n\n**Definition 1.1**:\nA probabilistic brownian motion is a process on $[0,\\infty)$, whichi is Gaussian with mean function $EX_{t} = 0\\; \\text{for}\\; t \\geq 0$ and covariance function $Cov(X_{s}, X_{t}) = \\min\\{s,t\\}\\; \\text{for}\\; s,t \\in [0,\\infty)$.\n\n## Remarks:\n\nIn the initial stage of constructing Brownian motion, we merely defined the covariance structure it should satisfiy. But we do not know whether it really exists, or just a formalized ideal object.\n\nDepends on the covariance function, we can do simulation on grid, but it is just a discrete approximation on a finite grid, which means we do not know:\n\n- Is the sample path continuous?\n- Does it osillate violently ar arbitrary small scales?\n- Does the discrete approximation truly converge to a limiting object?\n\nNow we need to prove there is a true random object exists in some space, corresponding to this formal content.\n\n## Statement:\n\nWe find the covaraince structure $\\min\\{s,t\\} = <1_{[0,s]}, 1_{[0,t]}>_{L^{2}[0,\\infty)}$, which means the covariance of brownian motion is an inner product of two vectors in $L^{2}$ space.\n\nNow we can abstract the question to:\n\n**Definition 1.2**:\nOn a hilbert space, construct a Gaussian family $\\{W(h), h\\in \\mathbb{H}\\}$, satisfies\n\n$$\nE[W(h)] = 0\\quad Cov(W(h), W(g)) = <h,g>_{H}\n$$\n\nthis is the isonormal Guassian process.\n\nSo we transform the question of \"whether Brownian motion exists\" from path-level challenge into a natural result of Gaussian structure on a Hilbert space.\n\nNow we need to prove the existence of the Gaussian isonormal process.\n\n---\n\n## Proof:\n\nDefine $n \\in \\mathbb{N}$, $h \\in \\mathbb{H}$, we have $W^{n}_{h} = \\sum^{n}_{i=1} \\langle h, e_{i} \\rangle X_{i}$, where $\\{e_{i}\\}$ is the orthonormal basis.\n\nTHe individual terms in these sums are independent Gaussian variable, with mean zero. Using **Parseval's formula**, we find that\n\n$$\n\\sum^{\\infty}_{i=1} E(\\langle h, e_{i} \\rangle X_{i})^{2} = \\sum^{\\infty}_{i=1} \\langle h, e_{i} \\rangle^{2} = ||h||^{2} \\lt \\infty\n$$\n\nAs this is finite, we can apply **Khintchine-Kolmogorov's theorem**, so there exists a limit variable, such that $W^{n}_{h} \\rightarrow W_{h}\\; a.s.\\quad \\text{and in}\\; \\mathcal{L}^{2}$.\n\nThen we can get a process $\\mathbb{W} = (W_{h})_{h \\in \\mathbb{H}}$. We claim that this process is isonormal.\n\nWe have $E W_{h} = \\lim\\limits_{n \\rightarrow \\infty} EW^{n}_{h} = 0$ and $EW^{2}_{h} = \\lim\\limits_{n \\rightarrow \\infty}E(W^{n}_{h})^{2} = ||h||^{2}$.\n\nFrom $W^{n}_{h} \\rightarrow W_{h}\\; a.s.$, we know $W^{n}_{h} \\stackrel{\\mathcal{D}}{\\rightarrow} W_{h}$, and from example 6.5, we know that $W^{n}_{h} \\stackrel{\\mathcal{D}}{\\rightarrow} \\mathcal{N}(0, ||h||^{2})$. then $W_{h} \\rightarrow \\mathcal{N}(0, ||h||^{2})$.\n\nNext, to observe the map: $h \\mapsto W_{h}$ is linear, which means\n\n$$\nW_{h_{1}} + W_{h_{2}} = W_{h_{1} + h_{2}}\\quad c \\cdot W_{h} = W_{c \\cdot h}\n$$\n\nWe can prove this by calculation.\n\nDepends on the linear combination $\\sum^{k}_{j = 1}c_{j}W_{h_{j}} = W_{\\sum^{k}_{j=1}c_{j}h_{j}}$, the process is Gaussian as desired.\n\nNow about the covariance structure, we know: $Cov(X,Y) = \\frac{1}{2}(V(X+Y) - V(X) - V(Y))$. Then we can have:\n\n$$\n\\begin{aligned}\nCov(W_{h},W_{g}) &= \\frac{1}{2}(V(W_{h+g}) - V(W_{h}) - V(W_{g}))\\\\\n&= \\frac{1}{2}(||h+g||^{2} - ||h||^{2} - ||g||^{2})\\\\\n&= \\langle h, g \\rangle\\\\\n\\end{aligned}\n$$\n\nQ.E.D.\n---\n\n## Remark:\n\n1. Hilbert space:\n\n2. Parseval's ormula:\n\n3. Khintchine-Kolmogorov's theorem:\n\n4. Example 6.5:\n\n\n---\n\n## Statement\n\nBefore we prove the existence of Brownian motion, we need to introduce a definition of Baby - Brownian motion.\n\n**Definition 1.3**\nA (probabilistic) baby - Brownian motion is a Guassian process $\\mathbb{Y}_{t} = (Y_{t})_{t \\in [0,1]}$ on $[0,1]$ with\n\n$$\nE Y_{t} = 0\\quad \\text{for}\\; t \\in [0,1] \\qquad Cov(Y_{s}, Y_{t}) = s\\; \\text{for}\\; 0 \\leq s \\leq t \\leq 1\n$$\n\nIf we have a baby - Brownian motion, we can define $X_{t} = \\frac{1}{1+t} Y_{\\frac{t}{1+t}} - tY_{1},\\; \\text{for} t \\in [0, \\infty)$. Because $\\frac{t}{1+t} \\in [0,1]$ and this is a linear combination, then $X_{t}$ is Brownian motion.\n\n**Theorem**:\nLet $(X_{n})_{n \\in \\mathbb{N}}$ be a sequence of independent $\\mathcal{N}(0,1)$ - distributed real-valued variables. Let $(e_{n})_{n \\in \\mathbb{N}}$ be an orthonormal basis of $L^{2}(\\mathbb{R}, \\mathbb{B}, \\lambda)$ where $\\lambda$ is the restriction of the Lebesgue measure to $[0,1]$. Then\n\n$$\nY_{t} = \\sum^{\\infty}_{i=1} \\langle 1_{[0,t]}, e_{i} \\rangle X_{i},\\quad \\text{for}\\; t \\in [0,1]\n$$\n\nis a well-defined probabilistic baby - Brownian motion.\n\n---\n\n## Proof:\n\nLet $\\mathbb{H} = L^{2}(\\mathbb{R}, \\mathbb{B}, \\lambda),\\; \\text{for}\\; t \\in [0,1]$, we let $h_{t} = 1_{[0,t]}$. Then $h_{t} \\in \\mathbb{H},\\; h_{t} = W_{h_{t}}$ where $\\mathbb{W} = (W_{h})_{h \\in \\mathbb{H}}$ is isonormal process.\n\nAs $\\mathbb{W}$ is Gaussian, then $\\mathbb{Y}$ is also Gaussian. we have\n\n$$\n\\begin{aligned}\nE Y_{t} &= E W_{h_{t}} = 0\\\\\nCov(Y_{s}, Y_{t}) &= Cov(W_{h_{s}}, W_{h_{t}}) = \\langle h_{s}, h_{t} \\rangle = \\int 1_{[0,s]} 1_{[0,t]} d \\lambda = \\int 1_{[0,s]} d \\lambda = s\\; (s \\leq t)\\\\\n\\end{aligned}\n$$\n\nso $\\mathbb{Y}$ is indeed a probabilistic baby - Brownian motion.\n\n---\n\n## Intuition:\n\n### Example 1: Brownian bridge\n\nBrwoian bridge is a Gaussian process $\\mathbb{Y} = (Y_{t})_{t \\in [0,1]}$ on $[0,1]$ with mean function\n\n$$\nE Y_{t} = 0\\quad \\text{for}\\; t \\in [0,1]\n$$\n\nand covariance function \n\n$$\nCov(Y_{s}, Y_{t}) = s(1-t)\\quad \\text{for}\\; 0 \\leq s \\leq t \\leq 1\n$$\n\n\nIn addtion to $Y_{0} =0\\; a.s.$, we have that $Y_{1} = 0\\; a.s.$. For this reason a Brownian motion is often reffered to as a tied-down Brownian motion.\n\n---\n\n## Remark:\n\nWe can generate a Brownian bridge from a baby - Brownian motion, if $\\mathbb{X} = (X_{t})_{t \\in [0,1]}$is a baby - Brownian motion, it is easy to see that\n\n$$\nY_{t} = X_{t} - tX_{1}\\quad \\text{for}\\; t \\in [0,1]\n$$\n\nwhere $\\mathbb{Y} = (Y_{t})_{t \\in [0,1]}$ is a Brownian bridge.\n\nSo if we have one of Brownian motion, baby-Brownian motion and Brownian bridge, we can generate the other two.","source":"_posts/BM_ques1.md","raw":"---\ntitle: Brownian Motion - Existence Proof\ndate: 2026-02-06\ncategories: [notes]\ntags:\n  - Probability\n  - Stochastic Processes\n  - Brownian Motion\ncourse: \n---\n\n# Brownian Motion\n\nExistence of a probabilistic Brownian motion\n\n<!-- more -->\n\n## Statement: Ê†∏ÂøÉÂÆö‰πâ (Statements)\n\nWe know the definition of Brownian motion as below:\n\n**Definition 1.1**:\nA probabilistic brownian motion is a process on $[0,\\infty)$, whichi is Gaussian with mean function $EX_{t} = 0\\; \\text{for}\\; t \\geq 0$ and covariance function $Cov(X_{s}, X_{t}) = \\min\\{s,t\\}\\; \\text{for}\\; s,t \\in [0,\\infty)$.\n\n## Remarks:\n\nIn the initial stage of constructing Brownian motion, we merely defined the covariance structure it should satisfiy. But we do not know whether it really exists, or just a formalized ideal object.\n\nDepends on the covariance function, we can do simulation on grid, but it is just a discrete approximation on a finite grid, which means we do not know:\n\n- Is the sample path continuous?\n- Does it osillate violently ar arbitrary small scales?\n- Does the discrete approximation truly converge to a limiting object?\n\nNow we need to prove there is a true random object exists in some space, corresponding to this formal content.\n\n## Statement:\n\nWe find the covaraince structure $\\min\\{s,t\\} = <1_{[0,s]}, 1_{[0,t]}>_{L^{2}[0,\\infty)}$, which means the covariance of brownian motion is an inner product of two vectors in $L^{2}$ space.\n\nNow we can abstract the question to:\n\n**Definition 1.2**:\nOn a hilbert space, construct a Gaussian family $\\{W(h), h\\in \\mathbb{H}\\}$, satisfies\n\n$$\nE[W(h)] = 0\\quad Cov(W(h), W(g)) = <h,g>_{H}\n$$\n\nthis is the isonormal Guassian process.\n\nSo we transform the question of \"whether Brownian motion exists\" from path-level challenge into a natural result of Gaussian structure on a Hilbert space.\n\nNow we need to prove the existence of the Gaussian isonormal process.\n\n---\n\n## Proof:\n\nDefine $n \\in \\mathbb{N}$, $h \\in \\mathbb{H}$, we have $W^{n}_{h} = \\sum^{n}_{i=1} \\langle h, e_{i} \\rangle X_{i}$, where $\\{e_{i}\\}$ is the orthonormal basis.\n\nTHe individual terms in these sums are independent Gaussian variable, with mean zero. Using **Parseval's formula**, we find that\n\n$$\n\\sum^{\\infty}_{i=1} E(\\langle h, e_{i} \\rangle X_{i})^{2} = \\sum^{\\infty}_{i=1} \\langle h, e_{i} \\rangle^{2} = ||h||^{2} \\lt \\infty\n$$\n\nAs this is finite, we can apply **Khintchine-Kolmogorov's theorem**, so there exists a limit variable, such that $W^{n}_{h} \\rightarrow W_{h}\\; a.s.\\quad \\text{and in}\\; \\mathcal{L}^{2}$.\n\nThen we can get a process $\\mathbb{W} = (W_{h})_{h \\in \\mathbb{H}}$. We claim that this process is isonormal.\n\nWe have $E W_{h} = \\lim\\limits_{n \\rightarrow \\infty} EW^{n}_{h} = 0$ and $EW^{2}_{h} = \\lim\\limits_{n \\rightarrow \\infty}E(W^{n}_{h})^{2} = ||h||^{2}$.\n\nFrom $W^{n}_{h} \\rightarrow W_{h}\\; a.s.$, we know $W^{n}_{h} \\stackrel{\\mathcal{D}}{\\rightarrow} W_{h}$, and from example 6.5, we know that $W^{n}_{h} \\stackrel{\\mathcal{D}}{\\rightarrow} \\mathcal{N}(0, ||h||^{2})$. then $W_{h} \\rightarrow \\mathcal{N}(0, ||h||^{2})$.\n\nNext, to observe the map: $h \\mapsto W_{h}$ is linear, which means\n\n$$\nW_{h_{1}} + W_{h_{2}} = W_{h_{1} + h_{2}}\\quad c \\cdot W_{h} = W_{c \\cdot h}\n$$\n\nWe can prove this by calculation.\n\nDepends on the linear combination $\\sum^{k}_{j = 1}c_{j}W_{h_{j}} = W_{\\sum^{k}_{j=1}c_{j}h_{j}}$, the process is Gaussian as desired.\n\nNow about the covariance structure, we know: $Cov(X,Y) = \\frac{1}{2}(V(X+Y) - V(X) - V(Y))$. Then we can have:\n\n$$\n\\begin{aligned}\nCov(W_{h},W_{g}) &= \\frac{1}{2}(V(W_{h+g}) - V(W_{h}) - V(W_{g}))\\\\\n&= \\frac{1}{2}(||h+g||^{2} - ||h||^{2} - ||g||^{2})\\\\\n&= \\langle h, g \\rangle\\\\\n\\end{aligned}\n$$\n\nQ.E.D.\n---\n\n## Remark:\n\n1. Hilbert space:\n\n2. Parseval's ormula:\n\n3. Khintchine-Kolmogorov's theorem:\n\n4. Example 6.5:\n\n\n---\n\n## Statement\n\nBefore we prove the existence of Brownian motion, we need to introduce a definition of Baby - Brownian motion.\n\n**Definition 1.3**\nA (probabilistic) baby - Brownian motion is a Guassian process $\\mathbb{Y}_{t} = (Y_{t})_{t \\in [0,1]}$ on $[0,1]$ with\n\n$$\nE Y_{t} = 0\\quad \\text{for}\\; t \\in [0,1] \\qquad Cov(Y_{s}, Y_{t}) = s\\; \\text{for}\\; 0 \\leq s \\leq t \\leq 1\n$$\n\nIf we have a baby - Brownian motion, we can define $X_{t} = \\frac{1}{1+t} Y_{\\frac{t}{1+t}} - tY_{1},\\; \\text{for} t \\in [0, \\infty)$. Because $\\frac{t}{1+t} \\in [0,1]$ and this is a linear combination, then $X_{t}$ is Brownian motion.\n\n**Theorem**:\nLet $(X_{n})_{n \\in \\mathbb{N}}$ be a sequence of independent $\\mathcal{N}(0,1)$ - distributed real-valued variables. Let $(e_{n})_{n \\in \\mathbb{N}}$ be an orthonormal basis of $L^{2}(\\mathbb{R}, \\mathbb{B}, \\lambda)$ where $\\lambda$ is the restriction of the Lebesgue measure to $[0,1]$. Then\n\n$$\nY_{t} = \\sum^{\\infty}_{i=1} \\langle 1_{[0,t]}, e_{i} \\rangle X_{i},\\quad \\text{for}\\; t \\in [0,1]\n$$\n\nis a well-defined probabilistic baby - Brownian motion.\n\n---\n\n## Proof:\n\nLet $\\mathbb{H} = L^{2}(\\mathbb{R}, \\mathbb{B}, \\lambda),\\; \\text{for}\\; t \\in [0,1]$, we let $h_{t} = 1_{[0,t]}$. Then $h_{t} \\in \\mathbb{H},\\; h_{t} = W_{h_{t}}$ where $\\mathbb{W} = (W_{h})_{h \\in \\mathbb{H}}$ is isonormal process.\n\nAs $\\mathbb{W}$ is Gaussian, then $\\mathbb{Y}$ is also Gaussian. we have\n\n$$\n\\begin{aligned}\nE Y_{t} &= E W_{h_{t}} = 0\\\\\nCov(Y_{s}, Y_{t}) &= Cov(W_{h_{s}}, W_{h_{t}}) = \\langle h_{s}, h_{t} \\rangle = \\int 1_{[0,s]} 1_{[0,t]} d \\lambda = \\int 1_{[0,s]} d \\lambda = s\\; (s \\leq t)\\\\\n\\end{aligned}\n$$\n\nso $\\mathbb{Y}$ is indeed a probabilistic baby - Brownian motion.\n\n---\n\n## Intuition:\n\n### Example 1: Brownian bridge\n\nBrwoian bridge is a Gaussian process $\\mathbb{Y} = (Y_{t})_{t \\in [0,1]}$ on $[0,1]$ with mean function\n\n$$\nE Y_{t} = 0\\quad \\text{for}\\; t \\in [0,1]\n$$\n\nand covariance function \n\n$$\nCov(Y_{s}, Y_{t}) = s(1-t)\\quad \\text{for}\\; 0 \\leq s \\leq t \\leq 1\n$$\n\n\nIn addtion to $Y_{0} =0\\; a.s.$, we have that $Y_{1} = 0\\; a.s.$. For this reason a Brownian motion is often reffered to as a tied-down Brownian motion.\n\n---\n\n## Remark:\n\nWe can generate a Brownian bridge from a baby - Brownian motion, if $\\mathbb{X} = (X_{t})_{t \\in [0,1]}$is a baby - Brownian motion, it is easy to see that\n\n$$\nY_{t} = X_{t} - tX_{1}\\quad \\text{for}\\; t \\in [0,1]\n$$\n\nwhere $\\mathbb{Y} = (Y_{t})_{t \\in [0,1]}$ is a Brownian bridge.\n\nSo if we have one of Brownian motion, baby-Brownian motion and Brownian bridge, we can generate the other two.","slug":"BM_ques1","published":1,"updated":"2026-02-07T09:28:54.439Z","comments":1,"layout":"post","photos":[],"_id":"cmlc7zo8e00058q0m3ak6hrgg","content":"<h1 id=\"brownian-motion\">Brownian Motion</h1>\n<p>Existence of a probabilistic Brownian motion</p>\n<span id=\"more\"></span>\n<h2 id=\"statement-Ê†∏ÂøÉÂÆö‰πâ-statements\">Statement: Ê†∏ÂøÉÂÆö‰πâ (Statements)</h2>\n<p>We know the definition of Brownian motion as below:</p>\n<p><strong>Definition 1.1</strong>: A probabilistic brownian motion is a process on <span class=\"math inline\">\\([0,\\infty)\\)</span>, whichi is Gaussian with mean function <span class=\"math inline\">\\(EX_{t} = 0\\; \\text{for}\\; t \\geq 0\\)</span> and covariance function <span class=\"math inline\">\\(Cov(X_{s}, X_{t}) = \\min\\{s,t\\}\\; \\text{for}\\; s,t \\in [0,\\infty)\\)</span>.</p>\n<h2 id=\"remarks\">Remarks:</h2>\n<p>In the initial stage of constructing Brownian motion, we merely defined the covariance structure it should satisfiy. But we do not know whether it really exists, or just a formalized ideal object.</p>\n<p>Depends on the covariance function, we can do simulation on grid, but it is just a discrete approximation on a finite grid, which means we do not know:</p>\n<ul>\n<li>Is the sample path continuous?</li>\n<li>Does it osillate violently ar arbitrary small scales?</li>\n<li>Does the discrete approximation truly converge to a limiting object?</li>\n</ul>\n<p>Now we need to prove there is a true random object exists in some space, corresponding to this formal content.</p>\n<h2 id=\"statement\">Statement:</h2>\n<p>We find the covaraince structure <span class=\"math inline\">\\(\\min\\{s,t\\} = &lt;1_{[0,s]}, 1_{[0,t]}&gt;_{L^{2}[0,\\infty)}\\)</span>, which means the covariance of brownian motion is an inner product of two vectors in <span class=\"math inline\">\\(L^{2}\\)</span> space.</p>\n<p>Now we can abstract the question to:</p>\n<p><strong>Definition 1.2</strong>: On a hilbert space, construct a Gaussian family <span class=\"math inline\">\\(\\{W(h), h\\in \\mathbb{H}\\}\\)</span>, satisfies</p>\n<p><span class=\"math display\">\\[\nE[W(h)] = 0\\quad Cov(W(h), W(g)) = &lt;h,g&gt;_{H}\n\\]</span></p>\n<p>this is the isonormal Guassian process.</p>\n<p>So we transform the question of ‚Äúwhether Brownian motion exists‚Äù from path-level challenge into a natural result of Gaussian structure on a Hilbert space.</p>\n<p>Now we need to prove the existence of the Gaussian isonormal process.</p>\n<hr />\n<h2 id=\"proof\">Proof:</h2>\n<p>Define <span class=\"math inline\">\\(n \\in \\mathbb{N}\\)</span>, <span class=\"math inline\">\\(h \\in \\mathbb{H}\\)</span>, we have <span class=\"math inline\">\\(W^{n}_{h} = \\sum^{n}_{i=1} \\langle h, e_{i} \\rangle X_{i}\\)</span>, where <span class=\"math inline\">\\(\\{e_{i}\\}\\)</span> is the orthonormal basis.</p>\n<p>THe individual terms in these sums are independent Gaussian variable, with mean zero. Using <strong>Parseval‚Äôs formula</strong>, we find that</p>\n<p><span class=\"math display\">\\[\n\\sum^{\\infty}_{i=1} E(\\langle h, e_{i} \\rangle X_{i})^{2} = \\sum^{\\infty}_{i=1} \\langle h, e_{i} \\rangle^{2} = ||h||^{2} \\lt \\infty\n\\]</span></p>\n<p>As this is finite, we can apply <strong>Khintchine-Kolmogorov‚Äôs theorem</strong>, so there exists a limit variable, such that <span class=\"math inline\">\\(W^{n}_{h} \\rightarrow W_{h}\\; a.s.\\quad \\text{and in}\\; \\mathcal{L}^{2}\\)</span>.</p>\n<p>Then we can get a process <span class=\"math inline\">\\(\\mathbb{W} = (W_{h})_{h \\in \\mathbb{H}}\\)</span>. We claim that this process is isonormal.</p>\n<p>We have <span class=\"math inline\">\\(E W_{h} = \\lim\\limits_{n \\rightarrow \\infty} EW^{n}_{h} = 0\\)</span> and <span class=\"math inline\">\\(EW^{2}_{h} = \\lim\\limits_{n \\rightarrow \\infty}E(W^{n}_{h})^{2} = ||h||^{2}\\)</span>.</p>\n<p>From <span class=\"math inline\">\\(W^{n}_{h} \\rightarrow W_{h}\\; a.s.\\)</span>, we know <span class=\"math inline\">\\(W^{n}_{h} \\stackrel{\\mathcal{D}}{\\rightarrow} W_{h}\\)</span>, and from example 6.5, we know that <span class=\"math inline\">\\(W^{n}_{h} \\stackrel{\\mathcal{D}}{\\rightarrow} \\mathcal{N}(0, ||h||^{2})\\)</span>. then <span class=\"math inline\">\\(W_{h} \\rightarrow \\mathcal{N}(0, ||h||^{2})\\)</span>.</p>\n<p>Next, to observe the map: <span class=\"math inline\">\\(h \\mapsto W_{h}\\)</span> is linear, which means</p>\n<p><span class=\"math display\">\\[\nW_{h_{1}} + W_{h_{2}} = W_{h_{1} + h_{2}}\\quad c \\cdot W_{h} = W_{c \\cdot h}\n\\]</span></p>\n<p>We can prove this by calculation.</p>\n<p>Depends on the linear combination <span class=\"math inline\">\\(\\sum^{k}_{j = 1}c_{j}W_{h_{j}} = W_{\\sum^{k}_{j=1}c_{j}h_{j}}\\)</span>, the process is Gaussian as desired.</p>\n<p>Now about the covariance structure, we know: <span class=\"math inline\">\\(Cov(X,Y) = \\frac{1}{2}(V(X+Y) - V(X) - V(Y))\\)</span>. Then we can have:</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nCov(W_{h},W_{g}) &amp;= \\frac{1}{2}(V(W_{h+g}) - V(W_{h}) - V(W_{g}))\\\\\n&amp;= \\frac{1}{2}(||h+g||^{2} - ||h||^{2} - ||g||^{2})\\\\\n&amp;= \\langle h, g \\rangle\\\\\n\\end{aligned}\n\\]</span></p>\n<h2 id=\"q.e.d.\">Q.E.D.</h2>\n<h2 id=\"remark\">Remark:</h2>\n<ol type=\"1\">\n<li><p>Hilbert space:</p></li>\n<li><p>Parseval‚Äôs ormula:</p></li>\n<li><p>Khintchine-Kolmogorov‚Äôs theorem:</p></li>\n<li><p>Example 6.5:</p></li>\n</ol>\n<hr />\n<h2 id=\"statement-1\">Statement</h2>\n<p>Before we prove the existence of Brownian motion, we need to introduce a definition of Baby - Brownian motion.</p>\n<p><strong>Definition 1.3</strong> A (probabilistic) baby - Brownian motion is a Guassian process <span class=\"math inline\">\\(\\mathbb{Y}_{t} = (Y_{t})_{t \\in [0,1]}\\)</span> on <span class=\"math inline\">\\([0,1]\\)</span> with</p>\n<p><span class=\"math display\">\\[\nE Y_{t} = 0\\quad \\text{for}\\; t \\in [0,1] \\qquad Cov(Y_{s}, Y_{t}) = s\\; \\text{for}\\; 0 \\leq s \\leq t \\leq 1\n\\]</span></p>\n<p>If we have a baby - Brownian motion, we can define <span class=\"math inline\">\\(X_{t} = \\frac{1}{1+t} Y_{\\frac{t}{1+t}} - tY_{1},\\; \\text{for} t \\in [0, \\infty)\\)</span>. Because <span class=\"math inline\">\\(\\frac{t}{1+t} \\in [0,1]\\)</span> and this is a linear combination, then <span class=\"math inline\">\\(X_{t}\\)</span> is Brownian motion.</p>\n<p><strong>Theorem</strong>: Let <span class=\"math inline\">\\((X_{n})_{n \\in \\mathbb{N}}\\)</span> be a sequence of independent <span class=\"math inline\">\\(\\mathcal{N}(0,1)\\)</span> - distributed real-valued variables. Let <span class=\"math inline\">\\((e_{n})_{n \\in \\mathbb{N}}\\)</span> be an orthonormal basis of <span class=\"math inline\">\\(L^{2}(\\mathbb{R}, \\mathbb{B}, \\lambda)\\)</span> where <span class=\"math inline\">\\(\\lambda\\)</span> is the restriction of the Lebesgue measure to <span class=\"math inline\">\\([0,1]\\)</span>. Then</p>\n<p><span class=\"math display\">\\[\nY_{t} = \\sum^{\\infty}_{i=1} \\langle 1_{[0,t]}, e_{i} \\rangle X_{i},\\quad \\text{for}\\; t \\in [0,1]\n\\]</span></p>\n<p>is a well-defined probabilistic baby - Brownian motion.</p>\n<hr />\n<h2 id=\"proof-1\">Proof:</h2>\n<p>Let <span class=\"math inline\">\\(\\mathbb{H} = L^{2}(\\mathbb{R}, \\mathbb{B}, \\lambda),\\; \\text{for}\\; t \\in [0,1]\\)</span>, we let <span class=\"math inline\">\\(h_{t} = 1_{[0,t]}\\)</span>. Then <span class=\"math inline\">\\(h_{t} \\in \\mathbb{H},\\; h_{t} = W_{h_{t}}\\)</span> where <span class=\"math inline\">\\(\\mathbb{W} = (W_{h})_{h \\in \\mathbb{H}}\\)</span> is isonormal process.</p>\n<p>As <span class=\"math inline\">\\(\\mathbb{W}\\)</span> is Gaussian, then <span class=\"math inline\">\\(\\mathbb{Y}\\)</span> is also Gaussian. we have</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nE Y_{t} &amp;= E W_{h_{t}} = 0\\\\\nCov(Y_{s}, Y_{t}) &amp;= Cov(W_{h_{s}}, W_{h_{t}}) = \\langle h_{s}, h_{t} \\rangle = \\int 1_{[0,s]} 1_{[0,t]} d \\lambda = \\int 1_{[0,s]} d \\lambda = s\\; (s \\leq t)\\\\\n\\end{aligned}\n\\]</span></p>\n<p>so <span class=\"math inline\">\\(\\mathbb{Y}\\)</span> is indeed a probabilistic baby - Brownian motion.</p>\n<hr />\n<h2 id=\"intuition\">Intuition:</h2>\n<h3 id=\"example-1-brownian-bridge\">Example 1: Brownian bridge</h3>\n<p>Brwoian bridge is a Gaussian process <span class=\"math inline\">\\(\\mathbb{Y} = (Y_{t})_{t \\in [0,1]}\\)</span> on <span class=\"math inline\">\\([0,1]\\)</span> with mean function</p>\n<p><span class=\"math display\">\\[\nE Y_{t} = 0\\quad \\text{for}\\; t \\in [0,1]\n\\]</span></p>\n<p>and covariance function</p>\n<p><span class=\"math display\">\\[\nCov(Y_{s}, Y_{t}) = s(1-t)\\quad \\text{for}\\; 0 \\leq s \\leq t \\leq 1\n\\]</span></p>\n<p>In addtion to <span class=\"math inline\">\\(Y_{0} =0\\; a.s.\\)</span>, we have that <span class=\"math inline\">\\(Y_{1} = 0\\; a.s.\\)</span>. For this reason a Brownian motion is often reffered to as a tied-down Brownian motion.</p>\n<hr />\n<h2 id=\"remark-1\">Remark:</h2>\n<p>We can generate a Brownian bridge from a baby - Brownian motion, if <span class=\"math inline\">\\(\\mathbb{X} = (X_{t})_{t \\in [0,1]}\\)</span>is a baby - Brownian motion, it is easy to see that</p>\n<p><span class=\"math display\">\\[\nY_{t} = X_{t} - tX_{1}\\quad \\text{for}\\; t \\in [0,1]\n\\]</span></p>\n<p>where <span class=\"math inline\">\\(\\mathbb{Y} = (Y_{t})_{t \\in [0,1]}\\)</span> is a Brownian bridge.</p>\n<p>So if we have one of Brownian motion, baby-Brownian motion and Brownian bridge, we can generate the other two.</p>\n","excerpt":"<h1 id=\"brownian-motion\">Brownian Motion</h1>\n<p>Existence of a probabilistic Brownian motion</p>","more":"<h2 id=\"statement-Ê†∏ÂøÉÂÆö‰πâ-statements\">Statement: Ê†∏ÂøÉÂÆö‰πâ (Statements)</h2>\n<p>We know the definition of Brownian motion as below:</p>\n<p><strong>Definition 1.1</strong>: A probabilistic brownian motion is a process on <span class=\"math inline\">\\([0,\\infty)\\)</span>, whichi is Gaussian with mean function <span class=\"math inline\">\\(EX_{t} = 0\\; \\text{for}\\; t \\geq 0\\)</span> and covariance function <span class=\"math inline\">\\(Cov(X_{s}, X_{t}) = \\min\\{s,t\\}\\; \\text{for}\\; s,t \\in [0,\\infty)\\)</span>.</p>\n<h2 id=\"remarks\">Remarks:</h2>\n<p>In the initial stage of constructing Brownian motion, we merely defined the covariance structure it should satisfiy. But we do not know whether it really exists, or just a formalized ideal object.</p>\n<p>Depends on the covariance function, we can do simulation on grid, but it is just a discrete approximation on a finite grid, which means we do not know:</p>\n<ul>\n<li>Is the sample path continuous?</li>\n<li>Does it osillate violently ar arbitrary small scales?</li>\n<li>Does the discrete approximation truly converge to a limiting object?</li>\n</ul>\n<p>Now we need to prove there is a true random object exists in some space, corresponding to this formal content.</p>\n<h2 id=\"statement\">Statement:</h2>\n<p>We find the covaraince structure <span class=\"math inline\">\\(\\min\\{s,t\\} = &lt;1_{[0,s]}, 1_{[0,t]}&gt;_{L^{2}[0,\\infty)}\\)</span>, which means the covariance of brownian motion is an inner product of two vectors in <span class=\"math inline\">\\(L^{2}\\)</span> space.</p>\n<p>Now we can abstract the question to:</p>\n<p><strong>Definition 1.2</strong>: On a hilbert space, construct a Gaussian family <span class=\"math inline\">\\(\\{W(h), h\\in \\mathbb{H}\\}\\)</span>, satisfies</p>\n<p><span class=\"math display\">\\[\nE[W(h)] = 0\\quad Cov(W(h), W(g)) = &lt;h,g&gt;_{H}\n\\]</span></p>\n<p>this is the isonormal Guassian process.</p>\n<p>So we transform the question of ‚Äúwhether Brownian motion exists‚Äù from path-level challenge into a natural result of Gaussian structure on a Hilbert space.</p>\n<p>Now we need to prove the existence of the Gaussian isonormal process.</p>\n<hr />\n<h2 id=\"proof\">Proof:</h2>\n<p>Define <span class=\"math inline\">\\(n \\in \\mathbb{N}\\)</span>, <span class=\"math inline\">\\(h \\in \\mathbb{H}\\)</span>, we have <span class=\"math inline\">\\(W^{n}_{h} = \\sum^{n}_{i=1} \\langle h, e_{i} \\rangle X_{i}\\)</span>, where <span class=\"math inline\">\\(\\{e_{i}\\}\\)</span> is the orthonormal basis.</p>\n<p>THe individual terms in these sums are independent Gaussian variable, with mean zero. Using <strong>Parseval‚Äôs formula</strong>, we find that</p>\n<p><span class=\"math display\">\\[\n\\sum^{\\infty}_{i=1} E(\\langle h, e_{i} \\rangle X_{i})^{2} = \\sum^{\\infty}_{i=1} \\langle h, e_{i} \\rangle^{2} = ||h||^{2} \\lt \\infty\n\\]</span></p>\n<p>As this is finite, we can apply <strong>Khintchine-Kolmogorov‚Äôs theorem</strong>, so there exists a limit variable, such that <span class=\"math inline\">\\(W^{n}_{h} \\rightarrow W_{h}\\; a.s.\\quad \\text{and in}\\; \\mathcal{L}^{2}\\)</span>.</p>\n<p>Then we can get a process <span class=\"math inline\">\\(\\mathbb{W} = (W_{h})_{h \\in \\mathbb{H}}\\)</span>. We claim that this process is isonormal.</p>\n<p>We have <span class=\"math inline\">\\(E W_{h} = \\lim\\limits_{n \\rightarrow \\infty} EW^{n}_{h} = 0\\)</span> and <span class=\"math inline\">\\(EW^{2}_{h} = \\lim\\limits_{n \\rightarrow \\infty}E(W^{n}_{h})^{2} = ||h||^{2}\\)</span>.</p>\n<p>From <span class=\"math inline\">\\(W^{n}_{h} \\rightarrow W_{h}\\; a.s.\\)</span>, we know <span class=\"math inline\">\\(W^{n}_{h} \\stackrel{\\mathcal{D}}{\\rightarrow} W_{h}\\)</span>, and from example 6.5, we know that <span class=\"math inline\">\\(W^{n}_{h} \\stackrel{\\mathcal{D}}{\\rightarrow} \\mathcal{N}(0, ||h||^{2})\\)</span>. then <span class=\"math inline\">\\(W_{h} \\rightarrow \\mathcal{N}(0, ||h||^{2})\\)</span>.</p>\n<p>Next, to observe the map: <span class=\"math inline\">\\(h \\mapsto W_{h}\\)</span> is linear, which means</p>\n<p><span class=\"math display\">\\[\nW_{h_{1}} + W_{h_{2}} = W_{h_{1} + h_{2}}\\quad c \\cdot W_{h} = W_{c \\cdot h}\n\\]</span></p>\n<p>We can prove this by calculation.</p>\n<p>Depends on the linear combination <span class=\"math inline\">\\(\\sum^{k}_{j = 1}c_{j}W_{h_{j}} = W_{\\sum^{k}_{j=1}c_{j}h_{j}}\\)</span>, the process is Gaussian as desired.</p>\n<p>Now about the covariance structure, we know: <span class=\"math inline\">\\(Cov(X,Y) = \\frac{1}{2}(V(X+Y) - V(X) - V(Y))\\)</span>. Then we can have:</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nCov(W_{h},W_{g}) &amp;= \\frac{1}{2}(V(W_{h+g}) - V(W_{h}) - V(W_{g}))\\\\\n&amp;= \\frac{1}{2}(||h+g||^{2} - ||h||^{2} - ||g||^{2})\\\\\n&amp;= \\langle h, g \\rangle\\\\\n\\end{aligned}\n\\]</span></p>\n<h2 id=\"q.e.d.\">Q.E.D.</h2>\n<h2 id=\"remark\">Remark:</h2>\n<ol type=\"1\">\n<li><p>Hilbert space:</p></li>\n<li><p>Parseval‚Äôs ormula:</p></li>\n<li><p>Khintchine-Kolmogorov‚Äôs theorem:</p></li>\n<li><p>Example 6.5:</p></li>\n</ol>\n<hr />\n<h2 id=\"statement-1\">Statement</h2>\n<p>Before we prove the existence of Brownian motion, we need to introduce a definition of Baby - Brownian motion.</p>\n<p><strong>Definition 1.3</strong> A (probabilistic) baby - Brownian motion is a Guassian process <span class=\"math inline\">\\(\\mathbb{Y}_{t} = (Y_{t})_{t \\in [0,1]}\\)</span> on <span class=\"math inline\">\\([0,1]\\)</span> with</p>\n<p><span class=\"math display\">\\[\nE Y_{t} = 0\\quad \\text{for}\\; t \\in [0,1] \\qquad Cov(Y_{s}, Y_{t}) = s\\; \\text{for}\\; 0 \\leq s \\leq t \\leq 1\n\\]</span></p>\n<p>If we have a baby - Brownian motion, we can define <span class=\"math inline\">\\(X_{t} = \\frac{1}{1+t} Y_{\\frac{t}{1+t}} - tY_{1},\\; \\text{for} t \\in [0, \\infty)\\)</span>. Because <span class=\"math inline\">\\(\\frac{t}{1+t} \\in [0,1]\\)</span> and this is a linear combination, then <span class=\"math inline\">\\(X_{t}\\)</span> is Brownian motion.</p>\n<p><strong>Theorem</strong>: Let <span class=\"math inline\">\\((X_{n})_{n \\in \\mathbb{N}}\\)</span> be a sequence of independent <span class=\"math inline\">\\(\\mathcal{N}(0,1)\\)</span> - distributed real-valued variables. Let <span class=\"math inline\">\\((e_{n})_{n \\in \\mathbb{N}}\\)</span> be an orthonormal basis of <span class=\"math inline\">\\(L^{2}(\\mathbb{R}, \\mathbb{B}, \\lambda)\\)</span> where <span class=\"math inline\">\\(\\lambda\\)</span> is the restriction of the Lebesgue measure to <span class=\"math inline\">\\([0,1]\\)</span>. Then</p>\n<p><span class=\"math display\">\\[\nY_{t} = \\sum^{\\infty}_{i=1} \\langle 1_{[0,t]}, e_{i} \\rangle X_{i},\\quad \\text{for}\\; t \\in [0,1]\n\\]</span></p>\n<p>is a well-defined probabilistic baby - Brownian motion.</p>\n<hr />\n<h2 id=\"proof-1\">Proof:</h2>\n<p>Let <span class=\"math inline\">\\(\\mathbb{H} = L^{2}(\\mathbb{R}, \\mathbb{B}, \\lambda),\\; \\text{for}\\; t \\in [0,1]\\)</span>, we let <span class=\"math inline\">\\(h_{t} = 1_{[0,t]}\\)</span>. Then <span class=\"math inline\">\\(h_{t} \\in \\mathbb{H},\\; h_{t} = W_{h_{t}}\\)</span> where <span class=\"math inline\">\\(\\mathbb{W} = (W_{h})_{h \\in \\mathbb{H}}\\)</span> is isonormal process.</p>\n<p>As <span class=\"math inline\">\\(\\mathbb{W}\\)</span> is Gaussian, then <span class=\"math inline\">\\(\\mathbb{Y}\\)</span> is also Gaussian. we have</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nE Y_{t} &amp;= E W_{h_{t}} = 0\\\\\nCov(Y_{s}, Y_{t}) &amp;= Cov(W_{h_{s}}, W_{h_{t}}) = \\langle h_{s}, h_{t} \\rangle = \\int 1_{[0,s]} 1_{[0,t]} d \\lambda = \\int 1_{[0,s]} d \\lambda = s\\; (s \\leq t)\\\\\n\\end{aligned}\n\\]</span></p>\n<p>so <span class=\"math inline\">\\(\\mathbb{Y}\\)</span> is indeed a probabilistic baby - Brownian motion.</p>\n<hr />\n<h2 id=\"intuition\">Intuition:</h2>\n<h3 id=\"example-1-brownian-bridge\">Example 1: Brownian bridge</h3>\n<p>Brwoian bridge is a Gaussian process <span class=\"math inline\">\\(\\mathbb{Y} = (Y_{t})_{t \\in [0,1]}\\)</span> on <span class=\"math inline\">\\([0,1]\\)</span> with mean function</p>\n<p><span class=\"math display\">\\[\nE Y_{t} = 0\\quad \\text{for}\\; t \\in [0,1]\n\\]</span></p>\n<p>and covariance function</p>\n<p><span class=\"math display\">\\[\nCov(Y_{s}, Y_{t}) = s(1-t)\\quad \\text{for}\\; 0 \\leq s \\leq t \\leq 1\n\\]</span></p>\n<p>In addtion to <span class=\"math inline\">\\(Y_{0} =0\\; a.s.\\)</span>, we have that <span class=\"math inline\">\\(Y_{1} = 0\\; a.s.\\)</span>. For this reason a Brownian motion is often reffered to as a tied-down Brownian motion.</p>\n<hr />\n<h2 id=\"remark-1\">Remark:</h2>\n<p>We can generate a Brownian bridge from a baby - Brownian motion, if <span class=\"math inline\">\\(\\mathbb{X} = (X_{t})_{t \\in [0,1]}\\)</span>is a baby - Brownian motion, it is easy to see that</p>\n<p><span class=\"math display\">\\[\nY_{t} = X_{t} - tX_{1}\\quad \\text{for}\\; t \\in [0,1]\n\\]</span></p>\n<p>where <span class=\"math inline\">\\(\\mathbb{Y} = (Y_{t})_{t \\in [0,1]}\\)</span> is a Brownian bridge.</p>\n<p>So if we have one of Brownian motion, baby-Brownian motion and Brownian bridge, we can generate the other two.</p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cmlc7zo8e00058q0m3ak6hrgg","category_id":"cmlc7zo8f00068q0m23sh30ta","_id":"cmlc7zo8g00098q0megsl7b5t"}],"PostTag":[{"post_id":"cmlc7zo8e00058q0m3ak6hrgg","tag_id":"cmlc7zo8g00078q0m0o3fb2o7","_id":"cmlc7zo8g000b8q0m4ksgcvek"},{"post_id":"cmlc7zo8e00058q0m3ak6hrgg","tag_id":"cmlc7zo8g00088q0m7s510wye","_id":"cmlc7zo8g000c8q0me78o4ecf"},{"post_id":"cmlc7zo8e00058q0m3ak6hrgg","tag_id":"cmlc7zo8g000a8q0m7dxq7p2k","_id":"cmlc7zo8g000d8q0mb47p2dqr"}],"Tag":[{"name":"Probability","_id":"cmlc7zo8g00078q0m0o3fb2o7"},{"name":"Stochastic Processes","_id":"cmlc7zo8g00088q0m7s510wye"},{"name":"Brownian Motion","_id":"cmlc7zo8g000a8q0m7dxq7p2k"}]}}